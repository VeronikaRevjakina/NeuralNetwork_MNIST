{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Activation functions](https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid\n",
    "def sigmoid_f(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "def sigmoid_df(x):  \n",
    "    sig=sigmoid_f(x)\n",
    "    return sig*(1-sig)\n",
    "    \n",
    "#ReLU\n",
    "def ReLU_f_over(x):\n",
    "    return x*(x>0)\n",
    "\n",
    "def ReLU_f(x): \n",
    "    return np.where(x > 0, x, 0)\n",
    "\n",
    "\n",
    "def ReLU_df(x):\n",
    "    return np.where(x <= 0, 0, 1)\n",
    "\n",
    "#Softmax\n",
    "def softmax_my(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def softmax_df(x):  \n",
    "    soft=softmax_f(x)\n",
    "    return soft*(1-soft)\n",
    "\n",
    "def softmax_f(x):  # работает\n",
    "    temp = math.e**x\n",
    "    div = temp.sum(axis=1)\n",
    "    return np.transpose((np.divide(np.transpose(temp), np.transpose(div))))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "# Fetch MNIST dataset and create a local copy.\n",
    "if os.path.exists('mnist.npz'):\n",
    "    with np.load('mnist.npz', 'r') as data:\n",
    "        X = data['X']\n",
    "        y = data['y']\n",
    "else:\n",
    "    mnist = fetch_mldata(\"mnist-original\")\n",
    "    X, y = mnist.data / 255.0, mnist.target\n",
    "    np.savez('mnist.npz', X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(y.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Convert labels to One Hot Encoded\n",
    "num_digits = 10\n",
    "encoder = OneHotEncoder(categorical_features =[0]) \n",
    "y_one_hot = encoder.fit_transform(y.reshape(-1, 1)).toarray()\n",
    "print(y_one_hot[50000,:])\n",
    "\n",
    "# def to_one_hot(y): \n",
    "#     one_hot_y = np.zeros((len(y), num_digits))\n",
    "#     for i in range(len(y)):\n",
    "#         one_hot_y[i, int(one_hot_y[i])] = 1\n",
    "#     return one_hot_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X,y=shuffle(X,y)\n",
    "#Split into test , train based on Kaggle\n",
    "X_train=X[:60000,:]\n",
    "y_train=y_one_hot[:60000,:]\n",
    "\n",
    "X_test=X[60000:,:]\n",
    "y_test=y_one_hot[60000:,:]\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_initialization(size_prev, size_next):  # HE initialization for matrix weights\n",
    "    return np.random.randn(size_prev, size_next) * np.sqrt(2.0/size_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expects weights and bias to be a matrix and a vector, activation_function to be a value= 1,2 or 3\n",
    "#Use when need to store z, h at each layer (for deltas)\n",
    "def layer_predict(inputs, weights, bias, activation_function):\n",
    "    #main sum counting z\n",
    "    #bias as vector adds to each row of weight matrix\n",
    "    #example: weights (300, 784), inputs (10000,784), bias (300,) then out (10000, 300)\n",
    "    out = np.add(np.dot(weights, inputs.T).T, bias)\n",
    "    # return z and h=f(z), f activation as parameter\n",
    "    if activation_function == 1:\n",
    "        return out, sigmoid_f(out)\n",
    "    if activation_function == 2:\n",
    "        return out, ReLU_f(out)\n",
    "    if activation_function == 3:\n",
    "        return out, softmax_f(out)\n",
    "#Expects z to be a matrix , activation_function to be a value= 1,2 or 3\n",
    "#Use when counting delta back propagate using derivatives\n",
    "def layer_back_propagate_derivatives(z, activation_function):\n",
    "    if activation_function == 1:\n",
    "        return sigmoid_df(z)\n",
    "    if activation_function == 2:\n",
    "        return ReLU_df(z)\n",
    "    if activation_function == 3:\n",
    "        return softmax_df(z)\n",
    "    \n",
    "#Expects weights and bias to be a list of matrixes and a list of vectors \n",
    "#Use when given input data gets output without storing hidden layers values\n",
    "def neural_predict(x_data, weights_in, biases_in, neural_architecture):  \n",
    "    #for each layer in neural architecture (same dim as weights)\n",
    "    activation_functions=neural_architecture[:,2]\n",
    "    \n",
    "    for layer in range(0, len(neural_architecture)):  \n",
    "    \n",
    "        #initial layer then take x_data as input\n",
    "        if layer == 0:\n",
    "            z_temp, h_temp = layer_predict(x_data, weights_in[layer],biases_in[layer], activation_functions[layer])\n",
    "            h = h_temp\n",
    "        else:\n",
    "            #take h-previous layer output as input\n",
    "            z_temp, h_temp = layer_predict(h, weights_in[layer],biases_in[layer], activation_functions[layer])\n",
    "            h = h_temp\n",
    "\n",
    "    return h\n",
    "    \n",
    "def convert_to_binary(vector):\n",
    "    row_maxes = vector.max(axis=1).reshape(-1, 1)\n",
    "    vector[:] = np.where(vector == row_maxes, 1, 0)\n",
    "    return vector\n",
    "\n",
    "def root_mean_square_error(y_predict,y_actual):\n",
    "    y_predict=convert_to_binary(y_predict)\n",
    "    mse=np.sum(np.power(y_actual-y_predict,2))/len(y_predict)\n",
    "    return np.sqrt(mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch_mini_batch_gradient_step(X_train,y_train,w_weights,b_biases,neural_architecture,activation_functions,learning_rate,num_epoches,batch_size):\n",
    "    #A list of outputs of each layer\n",
    "    h =[]\n",
    "    #A list of weightned sum for each layer\n",
    "    z=[]\n",
    "    weights = w_weights.copy()\n",
    "    biases = b_biases.copy()\n",
    "    X_train,y_train=shuffle(X_train,y_train)\n",
    "    n_minibatches = X_train.shape[0] // batch_size #1875\n",
    "    i = 0\n",
    "    for i in range(n_minibatches):\n",
    "        #A list of outputs of each layer\n",
    "        h =[]\n",
    "        #A list of weightned sum for each layer\n",
    "        z=[]\n",
    "        deltas=[]\n",
    "        X_mini=X_train[i * batch_size:(i + 1)*batch_size, :] #(32,784)\n",
    "        y_mini=y_train[i * batch_size:(i + 1)*batch_size,:] #(32,10)\n",
    "        #print(np.shape(X_mini))   \n",
    "        #print(np.shape(y_mini)) \n",
    "        N=X_mini.shape[0] #32...\n",
    "        #PARAMETERS\n",
    "            \n",
    "        #Forward propagation\n",
    "        layers_amount=len(neural_architecture)\n",
    "        #weights=w_weights.copy()\n",
    "        #biases=b_biases.copy()\n",
    "        for layer in range(0, layers_amount):\n",
    "            #For initial layer take X as input\n",
    "            if layer==0:\n",
    "                inputs=X_mini\n",
    "                #z_temp, h_temp= layer_predict(X_mini, weights[layer], biases[layer], activation_functions[layer])\n",
    "            #For any other layer take previous layer output as input\n",
    "            else:\n",
    "                inputs=h[layer-1]\n",
    "                #z_temp, h_temp= layer_predict(h[layer-1], weights[layer], biases[layer], activation_functions[layer])\n",
    "            #Store as lists\n",
    "            z_temp, h_temp= layer_predict(inputs, weights[layer], biases[layer], activation_functions[layer])\n",
    "            z.append(z_temp) #0=(32,300) 1=(32,100) 2=(32,10)\n",
    "            h.append(h_temp)\n",
    "                \n",
    "        #y_predict=neural_predict(X_mini,weights,biases,neural_architecture) same as h[layers_amount - 1]\n",
    "        # z,y_predict=layer_predict(h[layer_amount-1],weights[layer_amount-1],biases[layer_amount-1],\n",
    "        #activation_functions[layer_amount-1])\n",
    "        #error=y_mini-y_predict \n",
    "            \n",
    "        #DELTA INITIAL AND HIDDEN HERE\n",
    "            \n",
    "        #Initialize empty list size as amount of layers in network\n",
    "        deltas= np.empty(layers_amount, dtype=object)\n",
    "        #Count last delta based on (1)\n",
    "        #(32,10)\n",
    "        delta_last = np.multiply((h[layers_amount - 1] - y_mini),layer_back_propagate_derivatives(z[layers_amount - 1], activation_functions[layers_amount - 1])) \n",
    "        #Store last delta in list\n",
    "#         print(\"Delta Last\")\n",
    "#         print(h[layers_amount - 1])\n",
    "#         print(z[layers_amount - 1])\n",
    "        deltas[layers_amount-1]=delta_last # matrix size (dataset_size,amount_of_neurons_on_this_layer) transpose weights\n",
    "        #Back propagating with step=1 , stop=-1 (need count on layer=0) \n",
    "        for layer in range(layers_amount-2,-1,-1):\n",
    "            #Counting delta on hidden layers based on (2)\n",
    "            deltas[layer]=np.multiply((deltas[layer+1]).dot(weights[layer+1]),layer_back_propagate_derivatives(z[layer], activation_functions[layer]))    \n",
    "#         for layer in range(0, layers_amount):\n",
    "#             print(\"Deltas\")\n",
    "#             print(i)\n",
    "#             print(deltas[layer][::100])\n",
    "        #PARAMETERS UPDATES USING DELTAS\n",
    "        for layer in range(0, layers_amount):\n",
    "            #For initial layer take X as input\n",
    "            if layer==0:\n",
    "                inputs=X_mini\n",
    "            #For any other layer take previous layer output as input\n",
    "            else:\n",
    "                inputs=h[layer-1]\n",
    "            #update weights\n",
    "            lN=learning_rate/N\n",
    "            biases[layer]=biases[layer]-lN*np.sum(deltas[layer],axis=0)\n",
    "            weights[layer]=weights[layer]-lN*((deltas[layer].T).dot(inputs))\n",
    "#             print(\"Learned\")\n",
    "#             print(layer)\n",
    "#             print(learning_rate*np.sum(deltas[layer],axis=0)/N )\n",
    "#             print(layer)\n",
    "#             print(\"Weights\")\n",
    "            #print(learning_rate*((deltas[layer].T).dot(inputs))/N )\n",
    "#             print(learning_rate*((deltas[layer].T).dot(inputs))/N)\n",
    "    return weights,biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(X_train,y_train,neural_architecture,learning_rate,num_epoches,batch_size):\n",
    "    #A list of matrixes size of (new_layer_neurons_amount,prev_layer_neurons_amount)\n",
    "    w_weights=[]\n",
    "    #A list of vectors size of (new_layer_neurons_amount,1)\n",
    "    b_biases=[]\n",
    "    accuracy=[]\n",
    "   \n",
    "    for layer in range(0, len(neural_architecture)):\n",
    "        #For first time forward propagation random initialization\n",
    "        w_weights.append(np.full((neural_architecture[layer][1], neural_architecture[layer][0]), 0))\n",
    "        #w_weights.append(he_initialization(neural_architecture[layer,1],neural_architecture[layer,0]))#(300, 784) (100, 300) (10, 100)\n",
    "        b_biases.append(np.zeros(neural_architecture[layer][1])) #(300,) (100,) (10,)\n",
    "    #Just to store as list    \n",
    "    activation_functions=neural_architecture[:,2]\n",
    "#     print(\"Initial weights random\")\n",
    "#     print(w_weights[0][::100,::50])\n",
    "#     print(b_biases[0][::100])\n",
    "    it=0\n",
    "    for it in range(num_epoches):\n",
    "        #print(weights[0][::100,::50])\n",
    "        #print(biases[0][::100])\n",
    "        w1=w_weights\n",
    "        b1=b_biases\n",
    "        w_weights,b_biases=one_epoch_mini_batch_gradient_step(X_train,y_train,w_weights,b_biases,neural_architecture,activation_functions,learning_rate,num_epoches,batch_size)\n",
    "#         print(\"Epoches changed from initial?\")\n",
    "#         print(w_weights[0][::100,::50])\n",
    "#         print(b_biases[0][::100])\n",
    "        y_predict=neural_predict(X_train,w_weights,b_biases,neural_architecture)\n",
    "        convert_to_binary(y_predict)\n",
    "        print(y_predict[::10000])\n",
    "        print(it)\n",
    "        print(np.array_equal(w1, w_weights) )\n",
    "        print(np.array_equal(b1, b_biases) )\n",
    "        accuracy.append(accuracy_score(y_train,y_predict)*100)\n",
    "        print(accuracy)\n",
    "    return w_weights,b_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "0\n",
      "False\n",
      "False\n",
      "[9.751666666666667]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "1\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "2\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "3\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "4\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "5\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666]\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "6\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "7\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "8\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "9\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in greater\n",
      "  \n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in less_equal\n",
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:28: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_maximum(a, axis, None, out, keepdims, initial)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "10\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001, 0.0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "11\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001, 0.0, 0.0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "12\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001, 0.0, 0.0, 0.0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "13\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001, 0.0, 0.0, 0.0, 0.0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "14\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "15\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "16\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "17\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "18\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "19\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "20\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "21\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "22\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "23\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "24\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "25\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "26\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "27\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "28\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "29\n",
      "False\n",
      "False\n",
      "[9.751666666666667, 9.035, 9.751666666666667, 9.751666666666667, 9.915000000000001, 10.441666666666666, 11.236666666666666, 9.915000000000001, 9.035, 9.915000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "469.01992893218994\n"
     ]
    }
   ],
   "source": [
    "rmse=[]\n",
    "omega=[]\n",
    "omega0=[]\n",
    "rmse_train=[]\n",
    "\n",
    "#Already initialized\n",
    "#X_test (10000, 784)\n",
    "#y_test (10000, 10)\n",
    "\n",
    "#X_train (60000, 784)\n",
    "#y_train (60000, 10)\n",
    "      \n",
    "start = time.time()\n",
    "    \n",
    "neural_architecture= np.array([[784, 300, 2, 0],[300, 10, 3, 0]])\n",
    "\n",
    "weights_item,bias_item=mini_batch_gradient_descent(X_train,y_train,neural_architecture,learning_rate = 50,num_epoches=30,batch_size=32)\n",
    "stop = time.time()\n",
    "duration = stop-start\n",
    "print(duration)\n",
    "    \n",
    "        \n",
    "y_predict=neural_predict(X_test,weights_item,bias_item,neural_architecture)\n",
    "rmse.append(root_mean_square_error(y_predict,y_test))\n",
    "#NEED TO PLOT RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def mse_plot(rmse):  \n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.plot(rmse, label=\"MSE train\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "mse_plot(rmse)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas= np.empty(5, dtype=object)\n",
    "print(deltas)\n",
    "omega=np.full((3, 2), 0.5)\n",
    "deltas[2]=omega\n",
    "print(deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,y_test=shuffle(X_test,y_test)\n",
    "inputs=X_test[:100,:]  \n",
    "weights=[]\n",
    "neural_architecture= np.array([[784, 300, 1, 0],[300, 100, 1, 0],[100, 10, 3, 0]])\n",
    "activation_functions=neural_architecture[:,2]\n",
    "biases=[]\n",
    "#A list of outputs of each layer\n",
    "h =[]\n",
    "#A list of weightned sum for each layer\n",
    "z=[]\n",
    "deltas= np.zeros(len(neural_architecture), dtype=object)    \n",
    "for layer in range(0, len(neural_architecture)):\n",
    "    #For first time forward propagation random initialization\n",
    "    #weights.append(he_initialization(neural_architecture[layer,1],neural_architecture[layer,0]))\n",
    "    #weights.append(np.zeros(neural_architecture[layer][1],neural_architecture[layer][0]))\n",
    "    weights.append(np.full((neural_architecture[layer][1], neural_architecture[layer][0]), 0))\n",
    "    biases.append(np.zeros(neural_architecture[layer][1]))\n",
    "    #Just to store as list\n",
    "\n",
    "    \n",
    "out = np.add(np.dot(weights[0], inputs.T).T, biases[0])\n",
    "#print(np.shape(weights))\n",
    "#print( len(neural_architecture))\n",
    "#print(len(weights))\n",
    "\n",
    "z,y_predict=layer_predict(inputs,weights[0],biases[0],1)\n",
    "z,y2=layer_predict(y_predict,weights[1],biases[1],1)\n",
    "z,y3=layer_predict(y2,weights[2],biases[2],3)\n",
    "\n",
    "y3_2=neural_predict(inputs,weights, biases, neural_architecture)\n",
    "#print(np.shape(y3_2))\n",
    "#convert_to_binary(y_predict)\n",
    "#print(softmax_f(out3))\n",
    "layers_amount=len(neural_architecture)\n",
    "        #weights=w_weights.copy()\n",
    "        #biases=b_biases.copy()\n",
    "        #A list of outputs of each layer\n",
    "h =[]\n",
    "#A list of weightned sum for each layer\n",
    "z=[]\n",
    "for layer in range(0, layers_amount):\n",
    "            #For initial layer take X as input\n",
    "    if layer==0:\n",
    "        inputs1=X_test[:100,:]\n",
    "                #z_temp, h_temp= layer_predict(X_mini, weights[layer], biases[layer], activation_functions[layer])\n",
    "            #For any other layer take previous layer output as input\n",
    "    else:\n",
    "        inputs1=h[layer-1]\n",
    "                #z_temp, h_temp= layer_predict(h[layer-1], weights[layer], biases[layer], activation_functions[layer])\n",
    "            #Store as lists\n",
    "    z_temp, h_temp= layer_predict(inputs1, weights[layer], biases[layer], activation_functions[layer])\n",
    "    z.append(z_temp) #0=(32,300) 1=(32,100) 2=(32,10)\n",
    "    h.append(h_temp)\n",
    "#print(np.shape(h[layers_amount - 1] - y_test[:100,:]))\n",
    "#print(np.shape(layer_back_propagate_derivatives(z[layers_amount - 1], activation_functions[layers_amount - 1])))\n",
    "delta_last = np.multiply((h[layers_amount - 1] - y_test[:100,:]),layer_back_propagate_derivatives(z[layers_amount - 1], activation_functions[layers_amount - 1]))\n",
    "#print( h[layers_amount - 1] - y_test[:100,:])\n",
    "#print(delta_last)\n",
    "deltas[layers_amount-1]=delta_last\n",
    "\n",
    "for layer in range(layers_amount-2,-1,-1):\n",
    "    #Counting delta on hidden layers based on (2)\n",
    "    #print(np.shape(layer_back_propagate_derivatives(z[layer], activation_functions[layer])))\n",
    "    deltas[layer]=np.multiply((deltas[layer+1]).dot(weights[layer+1]),layer_back_propagate_derivatives(z[layer], activation_functions[layer])) \n",
    "\n",
    "print(np.shape(deltas[2]))\n",
    "\n",
    "for layer in range(0, layers_amount):\n",
    "    #For initial layer take X as input\n",
    "    if layer==0:\n",
    "        inputs2=X_test[:100,:]\n",
    "    #For any other layer take previous layer output as input\n",
    "    else:\n",
    "        inputs2=h[layer-1]\n",
    "                #update weights\n",
    "    N=100\n",
    "    #print(np.shape(weights[layer]))\n",
    "    biases[layer]=biases[layer]-0.001*np.sum(deltas[layer],axis=0)/N\n",
    "    weights[layer]=weights[layer]-0.001*((deltas[layer].T).dot(inputs2))/N\n",
    "    #print(weights[layer])\n",
    "print(deltas[2].T)\n",
    "print(h[1])\n",
    "print((deltas[2].T).dot(h[1]))\n",
    "print(weights[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer=0\n",
    "for layer in range(len(neural_architecture)):\n",
    "    #print(np.shape(weights[layer]))\n",
    "    print(len(neural_architecture))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array([[0.5, 0.5, 0.5, 0.5,0.5,0.5, 0.5, 0.5, 0.5,0.5],\n",
    "             [0.5, 0.5, 0.5, 0.5,0.5,0.5, 0.5, 0.5, 0.5,0.5],\n",
    "             [0.5, 0.5, 0.5, 0.5,0.5,0.5, 0.5, 0.5, 0.5,0.5]])\n",
    "print(softmax_f(xs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[::2000,::50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(-5.62601022e-051 )\n",
    "numbers = [1.74408899e-012, 2.37943956e-004, 2.63039636e-006, 2.55056039e-010,\n",
    "  3.21408160e-051, 9.99759377e-001, 2.54537571e-008, 2.25822913e-008,\n",
    "  2.00556809e-037, 2.19572601e-128]\n",
    "\n",
    "for number in numbers:\n",
    "    print(f'{number:9.51f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
