{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Activation functions](https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "def deriv_sigmoid(x):  \n",
    "    sig=sigmoid(x)\n",
    "    return sig*(1-sig)\n",
    "    \n",
    "#ReLU\n",
    "def ReLU(x):\n",
    "    return x*(x>0)\n",
    "\n",
    "def deriv_ReLU(x):\n",
    "    return np.where(x <= 0, 0, 1)\n",
    "\n",
    "#Softmax\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def deriv_softmax(x):  \n",
    "    soft=softmax(x)\n",
    "    return soft*(1-soft)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "# Fetch MNIST dataset and create a local copy.\n",
    "if os.path.exists('mnist.npz'):\n",
    "    with np.load('mnist.npz', 'r') as data:\n",
    "        X = data['X']\n",
    "        y = data['y']\n",
    "else:\n",
    "    mnist = fetch_mldata(\"mnist-original\")\n",
    "    X, y = mnist.data / 255.0, mnist.target\n",
    "    np.savez('mnist.npz', X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(y.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Convert labels to One Hot Encoded\n",
    "num_digits = 10\n",
    "encoder = OneHotEncoder(categorical_features =[0]) \n",
    "y_one_hot = encoder.fit_transform(y.reshape(-1, 1)).toarray()\n",
    "print(y_one_hot[50000,:])\n",
    "\n",
    "# def to_one_hot(y): \n",
    "#     one_hot_y = np.zeros((len(y), num_digits))\n",
    "#     for i in range(len(y)):\n",
    "#         one_hot_y[i, int(one_hot_y[i])] = 1\n",
    "#     return one_hot_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X,y=shuffle(X,y)\n",
    "#Split into test , train based on Kaggle\n",
    "X_train=X[:60000,:]\n",
    "y_train=y_one_hot[:60000,:]\n",
    "\n",
    "X_test=X[60000:,:]\n",
    "y_test=y_one_hot[60000:,:]\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_initialization(size_prev, size_next):  # HE initialization for matrix weights\n",
    "    return np.random.randn(size_prev, size_next) * np.sqrt(2.0/size_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expects weights and bias to be a matrix and a vector, activation_function to be a value= 1,2 or 3\n",
    "#Use when need to store z, h at each layer (for deltas)\n",
    "def layer_predict(inputs, weights, bias, activation_function):\n",
    "    #main sum counting z\n",
    "    #bias as vector adds to each row of weight matrix\n",
    "    #example: weights (300, 784), inputs (10000,784), bias (300,) then out (10000, 300)\n",
    "    out = np.add(np.dot(weights, inputs.T).T, bias)\n",
    "    # return z and h=f(z), f activation as parameter\n",
    "    if activation_function == 1:\n",
    "        return out, sigmoid(out)\n",
    "    if activation_function == 2:\n",
    "        return out, ReLU(out)\n",
    "    if activation_function == 3:\n",
    "        return out, softmax(out)\n",
    "#Expects z to be a matrix , activation_function to be a value= 1,2 or 3\n",
    "#Use when counting delta back propagate using derivatives\n",
    "def layer_back_propagate_derivatives(z, activation_function):\n",
    "    if activation_function == 1:\n",
    "        return deriv_sigmoid(z)\n",
    "    if activation_function == 2:\n",
    "        return deriv_ReLU(z)\n",
    "    if activation_function == 3:\n",
    "        return deriv_softmax(z)\n",
    "    \n",
    "#Expects weights and bias to be a list of matrixes and a list of vectors \n",
    "#Use when given input data gets output without storing hidden layers values\n",
    "def neural_predict(x_data, weights, bias, neural_architecture):  \n",
    "    #for each layer in neural architecture (same dim as weights)\n",
    "    activation_functions=neural_architecture[:,2]\n",
    "    \n",
    "    for layer in range(0, len(neural_architecture)):  # forward one batch\n",
    "    \n",
    "        #initial layer then take x_data as input\n",
    "        if layer == 0:\n",
    "            z_temp, h_temp = layer_predict(x_data, weights[layer],biases[layer], activation_functions[layer])\n",
    "            h = h_temp\n",
    "        else:\n",
    "            #take h-previous layer output as input\n",
    "            z_temp, h_temp = layer_predict(h, weights[layer],biases[layer], activation_functions[layer])\n",
    "            h = h_temp\n",
    "\n",
    "    return h\n",
    "    \n",
    "def convert_to_binary(vector):\n",
    "    row_maxes = vector.max(axis=1).reshape(-1, 1)\n",
    "    vector[:] = np.where(vector == row_maxes, 1, 0)\n",
    "    return vector\n",
    "\n",
    "def root_mean_square_error(y_predict,y_actual):\n",
    "    y_predict=convert_to_binary(y_predict)\n",
    "    mse=np.sum(np.power(y_actual-y_predict,2))/len(y_predict)\n",
    "    return np.sqrt(mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(X_train,y_train,neural_architecture,learning_rate,num_epoches,batch_size):\n",
    "    #A list of matrixes size of (new_layer_neurons_amount,prev_layer_neurons_amount)\n",
    "    weights=[]\n",
    "    #A list of vectors size of (new_layer_neurons_amount,1)\n",
    "    biases=[]\n",
    "   \n",
    "    for layer in range(0, len(neural_architecture)):\n",
    "        #For first time forward propagation random initialization\n",
    "        weights.append(he_initialization(neural_architecture[layer,1],neural_architecture[layer,0]))#(300, 784) (100, 300) (10, 100)\n",
    "        biases.append(np.zeros(neural_architecture[layer][1])) #(300,) (100,) (10,)\n",
    "        #Just to store as list\n",
    "        activation_functions=neural_architecture[:,2]\n",
    "        \n",
    "    it=0\n",
    "    for it in range(num_epoches):\n",
    "        #A list of outputs of each layer\n",
    "        h =[]\n",
    "        #A list of weightned sum for each layer\n",
    "        z=[]\n",
    "    \n",
    "        X_train,y_train=shuffle(X_train,y_train)\n",
    "        n_minibatches = X_train.shape[0] // batch_size #1875\n",
    "        i = 0\n",
    "        for i in range(n_minibatches):\n",
    "            X_mini=X_train[i * batch_size:(i + 1)*batch_size, :] #(32,784)\n",
    "            y_mini=y_train[i * batch_size:(i + 1)*batch_size,:] #(32,10)\n",
    "            #print(np.shape(X_mini))   \n",
    "            #print(np.shape(y_mini)) \n",
    "            N=X_mini.shape[0] #32\n",
    "            #PARAMETERS\n",
    "            \n",
    "            #Forward propagation\n",
    "            layers_amount=len(neural_architecture)\n",
    "            for layer in range(0, layers_amount):\n",
    "                #For initial layer take X as input\n",
    "                if layer==0:\n",
    "                    inputs=X_mini\n",
    "                    #z_temp, h_temp= layer_predict(X_mini, weights[layer], biases[layer], activation_functions[layer])\n",
    "                #For any other layer take previous layer output as input\n",
    "                else:\n",
    "                    inputs=h[layer-1]\n",
    "                    #z_temp, h_temp= layer_predict(h[layer-1], weights[layer], biases[layer], activation_functions[layer])\n",
    "                #Store as lists\n",
    "                z_temp, h_temp= layer_predict(inputs, weights[layer], biases[layer], activation_functions[layer])\n",
    "                z.append(z_temp) #0=(32,300) 1=(32,100) 2=(32, 10)\n",
    "                h.append(h_temp)\n",
    "                \n",
    "            #y_predict=neural_predict(X_mini,weights,biases,neural_architecture) same as h[layers_amount - 1]\n",
    "            # z,y_predict=layer_predict(h[layer_amount-1],weights[layer_amount-1],biases[layer_amount-1],\n",
    "            #activation_functions[layer_amount-1])\n",
    "            #error=y_mini-y_predict \n",
    "            \n",
    "            #DELTA INITIAL AND HIDDEN HERE\n",
    "            \n",
    "            #Initialize empty list size as amount of layers in network\n",
    "            deltas= np.empty(layers_amount, dtype=object)\n",
    "            #Count last delta based on (1)\n",
    "            #(32,10)\n",
    "            delta_last = np.multiply((h[layers_amount - 1] - y_mini),layer_back_propagate_derivatives(z[layers_amount - 1], activation_functions[layers_amount - 1])) \n",
    "            #Store last delta in list                                                                               \n",
    "            deltas[layers_amount-1]=delta_last # matrix size (dataset_size,amount_of_neurons_on_this_layer) transpose weights\n",
    "            #Back propagating with step=1 , stop=-1 (need count on layer=0) \n",
    "            for layer in range(layers_amount-2,-1,-1):\n",
    "                #Counting delta on hidden layers based on (2)\n",
    "                deltas[layer]=np.multiply((deltas[layer+1]).dot(weights[layer+1]),layer_back_propagate_derivatives(z[layer], activation_functions[layer]))    \n",
    "            #PARAMETERS UPDATES USING DELTAS\n",
    "            for layer in range(0, layers_amount):\n",
    "                #For initial layer take X as input\n",
    "                if layer==0:\n",
    "                    inputs=X_mini\n",
    "                #For any other layer take previous layer output as input\n",
    "                else:\n",
    "                    inputs=h[layer-1]\n",
    "                #update weights\n",
    "                biases[layer]=biases[layer]-learning_rate*np.sum(deltas[layer],axis=0)/N\n",
    "                weights[layer]=weights[layer]-learning_rate*((deltas[layer].T).dot(inputs))/N\n",
    "                #biases[layer]=biases[layer]-learning_rate*np.sum(deltas[layer],axis=0)\n",
    "                #biases[layer]=biases[layer]-0.001*np.sum(deltas[layer],axis=0)\n",
    "        y_predict=neural_predict(X_train,weights,biases,neural_architecture)\n",
    "        rmse.append(root_mean_square_error(y_predict,y_train))\n",
    "        print(rmse)\n",
    "    return weights,biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3426342266852378]\n",
      "[1.3426342266852378, 1.3438253854823055]\n",
      "[1.3426342266852378, 1.3438253854823055, 1.3430065772983641]\n",
      "[1.3426342266852378, 1.3438253854823055, 1.3430065772983641, 1.3422369388450015]\n",
      "[1.3426342266852378, 1.3438253854823055, 1.3430065772983641, 1.3422369388450015, 1.3428328265275615]\n",
      "[1.3426342266852378, 1.3438253854823055, 1.3430065772983641, 1.3422369388450015, 1.3428328265275615, 1.342994167274502]\n",
      "[1.3426342266852378, 1.3438253854823055, 1.3430065772983641, 1.3422369388450015, 1.3428328265275615, 1.342994167274502, 1.3431306712304651]\n",
      "[1.3426342266852378, 1.3438253854823055, 1.3430065772983641, 1.3422369388450015, 1.3428328265275615, 1.342994167274502, 1.3431306712304651, 1.3426218132196919]\n",
      "[1.3426342266852378, 1.3438253854823055, 1.3430065772983641, 1.3422369388450015, 1.3428328265275615, 1.342994167274502, 1.3431306712304651, 1.3426218132196919, 1.341777428140251]\n",
      "[1.3426342266852378, 1.3438253854823055, 1.3430065772983641, 1.3422369388450015, 1.3428328265275615, 1.342994167274502, 1.3431306712304651, 1.3426218132196919, 1.341777428140251, 1.3420258318427902]\n",
      "[1.3426342266852378, 1.3438253854823055, 1.3430065772983641, 1.3422369388450015, 1.3428328265275615, 1.342994167274502, 1.3431306712304651, 1.3426218132196919, 1.341777428140251, 1.3420258318427902, 1.342261772780059]\n",
      "[1.3426342266852378, 1.3438253854823055, 1.3430065772983641, 1.3422369388450015, 1.3428328265275615, 1.342994167274502, 1.3431306712304651, 1.3426218132196919, 1.341777428140251, 1.3420258318427902, 1.342261772780059, 1.3423735198023934]\n",
      "[1.3426342266852378, 1.3438253854823055, 1.3430065772983641, 1.3422369388450015, 1.3428328265275615, 1.342994167274502, 1.3431306712304651, 1.3426218132196919, 1.341777428140251, 1.3420258318427902, 1.342261772780059, 1.3423735198023934, 1.3424976722512407]\n",
      "[1.3426342266852378, 1.3438253854823055, 1.3430065772983641, 1.3422369388450015, 1.3428328265275615, 1.342994167274502, 1.3431306712304651, 1.3426218132196919, 1.341777428140251, 1.3420258318427902, 1.342261772780059, 1.3423735198023934, 1.3424976722512407, 1.342969346882745]\n",
      "[1.3426342266852378, 1.3438253854823055, 1.3430065772983641, 1.3422369388450015, 1.3428328265275615, 1.342994167274502, 1.3431306712304651, 1.3426218132196919, 1.341777428140251, 1.3420258318427902, 1.342261772780059, 1.3423735198023934, 1.3424976722512407, 1.342969346882745, 1.3426714663932746]\n",
      "[1.3426342266852378, 1.3438253854823055, 1.3430065772983641, 1.3422369388450015, 1.3428328265275615, 1.342994167274502, 1.3431306712304651, 1.3426218132196919, 1.341777428140251, 1.3420258318427902, 1.342261772780059, 1.3423735198023934, 1.3424976722512407, 1.342969346882745, 1.3426714663932746, 1.3428824718989123]\n",
      "[1.3426342266852378, 1.3438253854823055, 1.3430065772983641, 1.3422369388450015, 1.3428328265275615, 1.342994167274502, 1.3431306712304651, 1.3426218132196919, 1.341777428140251, 1.3420258318427902, 1.342261772780059, 1.3423735198023934, 1.3424976722512407, 1.342969346882745, 1.3426714663932746, 1.3428824718989123, 1.3420630884326314]\n",
      "[1.3426342266852378, 1.3438253854823055, 1.3430065772983641, 1.3422369388450015, 1.3428328265275615, 1.342994167274502, 1.3431306712304651, 1.3426218132196919, 1.341777428140251, 1.3420258318427902, 1.342261772780059, 1.3423735198023934, 1.3424976722512407, 1.342969346882745, 1.3426714663932746, 1.3428824718989123, 1.3420630884326314, 1.3410567972063425]\n"
     ]
    }
   ],
   "source": [
    "rmse=[]\n",
    "omega=[]\n",
    "omega0=[]\n",
    "rmse_train=[]\n",
    "\n",
    "#Already initialized\n",
    "#X_test (10000, 784)\n",
    "#y_test (10000, 10)\n",
    "\n",
    "#X_train (60000, 784)\n",
    "#y_train (60000, 10)\n",
    "      \n",
    "start = time.time()\n",
    "    \n",
    "neural_architecture= np.array([[784, 300, 1, 0],[300, 100, 1, 0],[100, 10, 3, 0]])\n",
    "\n",
    "weights_item,bias_item=mini_batch_gradient_descent(X_train,y_train,neural_architecture,learning_rate = 0.01,num_epoches=30,batch_size=32)\n",
    "stop = time.time()\n",
    "duration = stop-start\n",
    "print(duration)\n",
    "    \n",
    "        \n",
    "y_predict=neural_predict(X_test,weights_item,bias_item,neural_architecture)\n",
    "rmse.append(root_mean_square_error(y_predict,y_test))\n",
    "#NEED TO PLOT RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_architecture= np.array([[784, 300, 1, 0],[300, 100, 1, 0],[100, 10, 3, 0]])\n",
    "activation_functions=neural_architecture[:,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas= np.empty(5, dtype=object)\n",
    "print(deltas)\n",
    "omega=np.full((3, 2), 0.5)\n",
    "deltas[2]=omega\n",
    "print(deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "inputs=X_test \n",
    "weights=[]\n",
    "\n",
    "biases=[]\n",
    "#A list of outputs of each layer\n",
    "h =[]\n",
    "#A list of weightned sum for each layer\n",
    "z=[]\n",
    "deltas= np.zeros(len(neural_architecture), dtype=object)    \n",
    "for layer in range(0, len(neural_architecture)):\n",
    "    #For first time forward propagation random initialization\n",
    "    weights.append(he_initialization(neural_architecture[layer,1],neural_architecture[layer,0]))\n",
    "    biases.append(np.zeros(neural_architecture[layer][1]))\n",
    "    #Just to store as list\n",
    "    activation_functions=neural_architecture[:,2]\n",
    "    biases[layer]=biases[layer]-0.001*np.sum(deltas[layer],axis=0)\n",
    "out = np.add(np.dot(weights[0], inputs.T).T, biases[0])\n",
    "print(np.shape(weights))\n",
    "print( len(neural_architecture))\n",
    "print(len(weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(100,)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "for layer in range(0, len(neural_architecture)):\n",
    "    #print(np.shape(weights[layer]))\n",
    "    print(np.shape(biases[layer]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
